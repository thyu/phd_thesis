\chapter{3-D Body Pose Estimation}
\label{chap/body}

\section{Introduction}
\label{sec/body/intro}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{fig/body/figure1_actionexplain.pdf} 
	\caption{Action detection helps 3D pose estimation by providing the spatiotemporal structure of actions.} 
	\label{fig/body/actionexplain}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{fig/body/figure2_transferexplain.pdf}
	\caption{Knowledge transfer capability of the proposed method} 
	\label{fig/body/transferexplain}
\end{figure}

% Done 
Articulated 3D human pose estimation (3D HPE) is a longstanding problem in computer vision with various applications such as video analysis, motion capture and surveillance.
3D HPE aims to infer a human pose, represented by joint positions or angles, from input images or videos. 

% Done -- maybe longer 
% First problem - uncontrolled environments 
Contemporary methods approach 3D pose estimation as a regression or a manifold learning scenario, 3D poses are estimated by learning a mapping from the feature space to the parametrised pose space. 
Learning a mapping between the two high-dimensional spaces is an essentially ill-posed problem \cite{Elgammal2004}. Hence, additional cues are needed to recover the correct pose from multiple solutions, such as feature correspondences between cameras, visual tracking of body parts, foreground segmentation and 3D scene information. 
Nevertheless, obtaining such extra cues is not trivial. 
Most of the above techniques require a well-controlled environment, including a clean background for segmentation, a calibrated multi-camera network or specialised hardware.   
As these ideal imaging conditions rarely exist in reality, the applicability of traditional 3D HPE is greatly limited. 
Furthermore, if changes are made to the imaging environments, the whole pose estimator has to be retrained, making the pose estimation algorithm not scalable. 

% Make introduction longer?   
% Occlusion 
% For example, the same human silhouette can be generated from many body positions. Thus the mapping must be multivalued, in general returning the parameters \phi of a distribution in \theta space, with \phi a function of the image x. 

\subsection{Contributions}

% Done 
To address the above issues, this chapter presents several new techniques to estimate 3D human poses for practical application, particularly in uncontrolled environments. 
A novel pose estimation framework, which incorporates action detection and 2D deformable part model, is introduced to video-based 3D human pose estimation. 
Two different random forest-based pose estimators complement each other by independently detecting body part locations and holistic 3D pose. The contributions of the proposed approach are threefold:  

% Done 
\subsubsection{3D pose from action detection} 
The analysis of human pose and action are two closely interrelated topics in computer vision. Although there exist initial studies in using human poses for recognising actions \eg \cite{Yao2012, Wang2012}, the opposite direction, \ie using actions to help pose analysis, 
is still an aspect that is overlooked by existing literature. 
In this work, human activities are considered as a collection of atomic, repeatable actions. 
Each action is a time series of instantaneous poses, with particular starting and ending poses and their transitions in-between. 
An action detection algorithm infers the current state of an action, which works as a strong prior for pose estimation.   

In this work, action detection is appied to extract spatiotemporal structures of human actions for pose estimation. A new action detection forest algorithm, an extension of Hough forest \cite{Gall2009}, is introduced to produce a global body pose hypothesis.   
In figure \ref{fig/body/actionexplain}, action detection categorises the action category and then estimates the relative space-time location of the recognised action class. In other words, action detection predicts the time-varying states of an action, which assists 3D pose estimation within the action's time span. Once an action is detected, the current pose hypothesis can be determined by computing the relative spatiotemporal displacements between the current observation and the action's preceding poses.   
On top of its many benefits to pose estimation, action detection does not impose extra restriction on the imaging environment. As discussed in chapter \ref{chap/act}, there already exist promising approaches for human action analysis, action detection is therefore a suitabe candidate to improve 3D HPE in realistic applications.   

% DOne 
\subsubsection{3D poses from deformable part model} 
Traditional 3D HPE algorithms rely on low-level appearance features, such as silhouettes, motion templates and shape descriptors \cite{Hogg1983, Rogez2012, Navaratnam2006, Pons-Moll2011, Sigal2012}. Poses are often inferred by learning a mapping from the feature space to the parameterised pose space directly, without considering the mid-level structure of human body.  
On the other hand, concerning 2D human pose estimation (2D HPE), detection of mid-level body parts is a critical process, as the output pose is constructed from the body parts detected, according to a predefined structure, such as an articulated model \cite{Felzenszwalb2000, Andriluka2009, Yang2011, Eichner2012}.

As a result, 2D part-based pose estimation techniques are applied to infer 3D articulated poses. 
In the proposed approach, holistic shape features, such as silhouettes, are replaced by a deformable part model (DPM), \eg \cite{Yang2011}, which detects mid-level 2D body parts from low-level features before inferring the corresponding 3D poses. 
Existing DPM algorithms perform well in realistic uncontrolled environments, they have shown promising robustness to cluttered backgrounds and appearance changes, \eg different clothing. 
On the other side, occlusions and view ambiguities of the DPM part detector are corrected by the 3D pose prior from action detection.    
% Don't know where to add??? 
The proposed method is therefore knowledge transferable, learned models can be reused in unseen environments without retraining (see figure \ref{fig/body/transferexplain}). 

% Done 
\subsubsection{Cross-modality pose estimation} 
Estimation of 3D human poses from 2D DPM is essentially a cross-modality problem, as 3D structure of a pose is inferred using the features extracted from its 2D appearance. 
Since the relationship between the two spaces are implicit, learning a robust regression model across two modalities becomes a challenging issue. 
Multi-dimensional, non-linear regression problems can be handled by learning a forest-based regressor \cite{Shotton2013}, which were applied in medical imaging \cite{Criminisi2011} and depthmap-based pose estimation \cite{Girshick2011}.  

In the proposed system, holistic pose hypothesis from action detection is refined by learning a series of joint-specific cross-modality regression forests. 
The cross-modality regression forests estimates 3D joint positions from detected 2D parts, producing a local pose estimate for each joint. 
The results from cross-modality regression forests, together with the global estimate from the above-mentioned action detection forest, are combined using a late-fusion scheme to optimise the final 3D pose estimation. 
The outputs of both forests, and their combined results, are formulated in a probabilistic framework. While existing methods yield a point estimate in the pose space, the proposed approach outputs joint positions as probability distributions in 3D space. 

% To the best of our knowledge, this is the first application of regression forest \cite{Criminisi2011} across modalities to 3D HPE problem.  
%Estimating 3D human pose is essentially a cross-modality regression problem: 
%\emph{3D structure} of a human pose is inferred using features extracted from its \emph{2D appearance} 
%We design a algorithm to refine 3D joint positions using cross-modality regression forest. 
% 3D joint positions are refined, from detected 2D parts and action categories, using a cross-modality regression forest. 

\section{Related work}
\label{sec/body/review}

\subsection{Early methods} 

% Part-based 2D pose analysis has been studied for decades, examples of 
Human pose estimation has been studied for decades, where most of the early approaches focused on estimating poses in 2D images, including pictorial structure \cite{Fischler1973} and template matching \cite{Ioffe1999}. These methods, however, lack an automatic part detector, which implies that manual labelling is required for both training and testing data. Hence, their potential applications are greatly limited.
On the other side, 3D human pose estimation is a more sophisicated task than its 2D counterpart due to occlusions and high dimensionality of the pose space. Nonetheless, various techniques have been investigated to estimate 3D poses from video data. For instance, \cite{Hogg1983} used image edges to infer the 3D pose of a walking person captured from a carefully controlled scene. 
Micilotta \etal \cite{Micilotta2006} used serveral appearance-based part detectors, \eg boosting detectors for face and hand, to compose a simple 3D pose of the target's upper body. 
Navaratnam \etal \cite{Navaratnam2006} designed a semi-supervised regression algorithm, where unlabelled training data were used to learn a Gaussian mixture based pose regressor, using shape-context features extracted from silhouettes. Two extensive literature reviews on traditional 3D human pose estimation were presented by Aggarwal and Cai \cite{Aggarwal1999} and Poppe \cite{Poppe2007}. 

%The spatial arrangements of body parts have been widely used in 2D human pose anaylsis. For example, Fischler and Elschlager \cite{Fischler1973} conceptualized \emph{pictorial structure} for modeling the spatial structure of human pose, Ioffe et al. \cite{IOffe1999} described body parts as a collection of image templates. 
%However, the effectiveness of these approaches were greatly limited due to the lack of robust detectors for different body parts, manual labelling of body parts was often required. 
%On the other hand, 3D pose estimation is more complicated than its 2D counterpart, due to occlusions and a high dimensionality in the pose space. Various methods have been developed: Hogg \cite{Hogg1983} used image edges to compute the 3D pose of a walking person from a carefully controlled environment. 
%Micilotta et al. \cite{Micilotta2006} used several part detectors, e.g. face and hand detectors, to infer a simple 3D pose of the person upper body. A regression algorithm was presented by Navaratnam et al. \cite{Navaratnam2006} to compute 3D poses from silhouettes. 
%\subsubsection{Recent approaches}

More recent techniques of human pose estimation are discussed below according to the techniques or representations used.

\subsection{3D poses from silhouette or depth image} 
Holistic shapes, silhouettes in particular, are common features for 3D pose estimation. 
Recent approaches achieve state-of-the-art performances by combining holistic shape features with new features or optimisation constraints, such as motion templates \cite{Rogez2012}, pedestrian detectors \cite{Andriluka2010}, shape-context \cite{Agarwal2006}. 
On the other hand, thanks to the introduction of the Kinect sensor, 2.5D depth image emerges as a new direction for 3D HPE. 
As depth information is provided in the depth 
Depth information of the scene is provided in the depth images, information and as it provides inherent depth information and object segmentation. 
For instance, Baak \etal \cite{Baak2011} presented a data-driven algorithm for real-time 3D pose estimation. A variant of Dijkstra's algorithm was introduced to extract holistic pose features efficiently from depth images, and then combined with local estimates that were computed using Hausdorff distance. Ye \etal \cite{Ye2011} matched a single depth image with a set of precomputed motion exemplars to estimate a holistic body configuration. The initial result was subsequently refined by fitting the pose configuration back to the testing depth image. 
Sun \etal \cite{Sun2012} estimated 3D pose configurations from patches of depth images using regression forests.  
Using a similar regression forest algorithm, Taylor \etal \cite{Taylor2012} performed 3D HPE by computing dense correspondences from an input depth image to a deformable 3D articulated model.  
However, the above methods require either specialised hardware (\eg Kinect sensor) or controlled environments to acquire compatible training or testing data.  
However, the acquisition of depth image is the major limitation of the above 3D HPE approaches. They either require specialised hardware, \eg Kinect, or calibrated stereo cameras to capture depth images. Additionally, depth images still do not handle occlusions and are sensitive to noise. 

%Recently, several depth-based 3D HPE approaches have been introduced, such as \cite{Baak2011}, \cite{Ye2011} and \cite{Sun2012}. 
% Several methods recognise 3D human poses from depth images, using techniques such as point cloud matching \cite{Baak2011, Ye2011} and random forest \cite{Taylor2012, Sun2012}.  

For instance, Bissacco \etal. \cite{Bissacco2007} proposed a boosting classifier to compute human poses from both silhouettes and motion features.  
A lantent structured model is described in \cite{Ionescu2011} to estimate 3D poses from silhouettes. 
Motion templates are also used in 3D pose estimation, \cite{Rogez2012} used a global motion template to recognize poses using a tree-shape ensemble of rejectors. 
Pons-Moll \etal \cite{Pons-Moll2011} presented a pose optimization algorithm from silhouettes captured from multiple cameras. 

%for the task.
%Guan et al. \cite{Guan2009} estimated human poses and shapes simultaneously from silhouettes, assisted by manual initializations.
%Recently, depth sensor has become a popular imaging technique for computer vision as it provides inherent depth information as well as object segmentation.  
%Several methods are proposed to estimate 3D human poses from depth images, using techniques such as point cloud matching \cite{Baak2011, Ye2011} and random forest \cite{Sun2012}. 
%However, the above depth-based approaches require an accurate segmentation to extract shape features from input data, thus specialized hardware and unconstrained environments are often required. 

%Done 
\subsection{Multiple-cameras approaches} 

Pose ambiguity is the central problem of 3D HPE. 
During data acquisition, 3D body poses are projected to 2D video frames or depth images, occluded parts are difficult to be recovered from the data, losing their 3D information about the scene. Hence, each observation can be explained by more than one 3D pose configurations. 

A standard approach to resolve the pose ambiguity issue is to maximise the field of view by capturing multiple images simultaneously using calibrated cameras, \eg \cite{Pons-Moll2011, Sigal2012, Yao2012}.   
Although they achieve excellent accuracy, potential applications are restricted to a fixed, calibrated multi-camera system. 
Furthermore, resolving the pose ambiguity from multiple views is, still, a sophisicated optimization problem. 
Existing solutions are often computationally expensive, which further limit their potential applications.   

\subsection{Action for pose estimation}
As mentioned in the previous section, the integration of action and pose, in particular action for pose, is still a largely unexplored area. This work seek to investigate the feasibility of applying action detection to facilitate 3D human pose estimation in uncontrolled and monocular videos. 
Early approaches that combines action and pose constrains include \cite{Yu2010} and \cite{Raja2011}.
The closest work to this idea is \cite{Yao2012} that uses action recognition to assist a multi-view 3D HPE algorithm. Separate regression models are trained; After an action is recognised, poses are inferred using the model of the action class estimated. 
Whilst action recognition is applied, the inputs are still images captured from a controlled, multi-camera environment. 
Instead, we perform action detection in video, by which we exploit the spatiotemporal structure of actions in addition to action class labels, to infer 3D poses.
%When the action of a person is detected in a video, it gives much information about spatiotemporal structure of that person, which is extrmely useful in the pose estimation task.  
%For instance, when a ``walking'' action is detected, it gives a strong \emph{spatial} prior to the pose estimator as outputs are limited to the ``walking'' poses. 
% However, the combination of pose estimation and action detection techniques is still a largely unexplored area. 
% \emph{temporal} used is obtained to support the pose estimator. 
%By tracing the current pose from the starting time of the action detected,
%Action recognition is performed by single images, action category labels are only used as a switch to different manifolds, where the 3D poses are estimated. 
%in uncontrolled single video. 
%a main object of our model is to estimate s by leveraging 
%has not utilized the inherit spatiotemporal structure of an \emph{action}--

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.35\linewidth]{fig/body/figure4.pdf}
	\caption{Graphical representation of the proposed model} 
	\label{fig/body/figure4gm}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\linewidth]{fig/body/figure3_overview.pdf}
	\caption{Overview of the proposed framework}
	\label{fig/body/overview}
\end{figure} 

\subsection{Part-based pose estimation} 
Various methods have been introduced built upon the original seminal model of pictorial structures \cite{Felzenszwalb2000, Felzenszwalb2010}. Recent extensions include new motion features \cite{Andriluka2009}, improved part-based model learning and inference \cite{Yang2011, Sun2012a}, and pre-processing techniques (\eg face detector) \cite{Eichner2012}. 

%Current state-of-the-arts usually build upon the success of traditional methods with new features or improved models. For instance, Sapp et al. \cite{Sapp2011} modelled pictorial structure in a more complicated graph by decomposing the it into smaller, stretchable components. 
% Similarly, an branch-and-bound algorithm was proposed in \cite{Sun2012a} to extend traditional pictorial structure beyond star-shaped or tree graphs. 
% using new features 
%In \cite{Hua2007}, visual tracking is combined with part detection in order to estimate articulated human pose. 
%Ardriluka et al. \cite{Andriluka2009} revisited traditional pictorial structure using dense shape context and adaboost algorithm. 
%Eichner et al. \cite{Eichner2012} presented a multi-phase algorithm to detect 2D body parts from unconstrained images using various clues such as face detection and graph cut algorithm. 

%Despite achieving remarkable performances, particularly in uncontrolled environments, 
%As  has shown a better performance than its 3D counterpart, especially in uncontrolled environments, it is suggested that similar techniques can be applied %to improve 3D pose estimation results. As a result, recent advances in 2D pose estimation, e.g. deformable part-based models %\cite{Felzenszwalb2010}, inspired a resurgence of part-based approaches for 3D human pose estimation.   
Recent advances in 2D human pose estimation methods, particularly in uncontrolled environments, have inspired a resurgence of part-based approaches for 3D pose estimation. 
While some approaches use manually labelled 2D parts to estimate 3D poses, \eg \cite{Wei2009, Ramakrishna2012}, 
%the poselet algorithm \cite{Bourdev2009} estimate a rough 3D pose by learning 2D part templates. 
2D deformable part models are also investigated in 3D human pose estimation, for instance, \cite{Andriluka2010} uses a pedestrian detector with deformable body parts to estimate rough 3D poses in street scenes, 
\cite{Simo-Serra2012} optimises multiple pose hypotheses from 2D DPM using inverse kinematics to estimate 3D pose in a single image.
In this work, we further investigate the use of 2D DPM in a multi-action 3D HPE scenario. 

%Consequently, multi-action 3D pose estimation with deformable part model is a topic of great research potential. 
%Simo-Serra et al. \cite{Simoserra2012} estimate 3D poses in an image using multiple hypotheses obtained from a 2D pictorial structure model. 
%In addition, part-based models are also used to infer poses of walking pedestrians in videos\cite{Andriluka2010}. Meanwhile, the poselet algorithm \cite{Bourdev2009} uses templates of body parts to estimate a rough 3D pose from an unconstrained image. The above part-based approaches demonstrate a greater flexibility than holistic-based methods. Hence, part-based 3D pose estimation in a multi-action scenario has become a topic with great research potential. 

%By applying suitable inverse kinematics constraints, 3D poses can be estimated accurately from images with manually labeled parts \cite{Wei2009, RamakrishnaECCV2012}. 
%Simo-Serra et al. \cite{Simoserra2012} estimate 3D poses in an image using multiple hypotheses obtained from a 2D pictorial structure model. In addition, part-based models are also used to infer poses of walking pedestrians in videos\cite{Andriluka2010}. Meanwhile, the poselet algorithm \cite{Bourdev2009} uses templates of body parts to estimate a rough 3D pose from an unconstrained image. The above part-based approaches demonstrate a greater flexibility than holistic-based methods. Hence, part-based 3D pose estimation in a multi-action scenario has become a topic with great research potential. 

\section{Overview}
\label{sec/body/method}

Figure \ref{fig/body/figure4gm} describes the graphical model of our 3D human pose estimation framework: action detection is performed to yield the rough 3D pose estimates, then cross-modality regression forests with the estimated action classes are applied to refine the 3D pose estimations. While full poses, \ie 3D coordinates of all $\njoints$ joints, are learnt in the action detection forest, one joint location is estimated per cross-modality regression forest. $\njoints$ regression forests are hence trained separately in the model. The conceptual data-flow of the proposed framework is illustrated in figure \ref{fig/body/overview}.

%used to refine
%After action detection, cross-modality regression forests are used to refine the rough 3D pose estimations given by the action detection process. 

\section{Feature Extraction}

In order to perform 3D pose estimation in different backgrounds and scenarios, input features are extracted using a 2D part-based model. 
Firstly, a deformable part model (DPM) \cite{Yang2011} is employed as an off-the-shelf method to extract body parts from input frames.
Unlike the traditional holistic silhouette-based methods, where background subtraction is preformed, our method based on body parts works at unseen and dynamic backgrounds. 
The DPM fires $\nhyp$ body part configurations per frame, each hypothesis contains $26$ locations of detected body parts as in \cite{Yang2011}. 
%hypotheses on configuration of $26$ body parts are obained per frame (the number of hypothese is dentoed ${\nhyp}$). 
Subsequently, the detected configurations are normalised with respect to the distance between the head and the waist part. 
Finally, a feature vector is computed per configuration by the pairwise distances among the normalised parts, hence the feature vector takes $325 (=26\times25/2)$ dimensions.

In the training process, every feature vector is associated with a ground truth 3D pose.
The Kinect sensor is used to acquire 3D poses simultaneously with the RGB video sequences. 
A 3D human pose is represented by the scale-normalised coordinates of the $\njoints$ joints detected by the Kinect sensor, an articulated model of $\njoints=15$ joints is used in this work. 
Every feature vector in the training dataset is assigned to the 3D pose detected and one of the $\nclass$ action categories, according to its corresponding frame and video. The feature vectors are denoted as $\fset = \{\feature_{\videoth\frameth\hypth}\}$, where $\feature_{\videoth\frameth\hypth}$ is the $\hypth$-th configuration detected in the $\frameth$-th frame of the $\videoth$-th training video. The corresponding class label and corresponding 3D pose
are defined as  $\aset = \{\action_{\videoth\frameth\hypth} | \action_{\videoth\frameth\hypth} \in 1,\dots,\nclass \}$ and $\pset = \{\bodypose_{\videoth\frameth\hypth} | \bodypose_{\videoth\frameth\hypth} \in \realnum^{3\njoints}\}$ respectively.  
Furthermore, 3D poses $\pset$ of each action category are compressed separately into low-dimensional vectors $\ppset = \{\_{\videoth\frameth\hypth}  | \pbodypose_{\videoth\frameth\hypth} \in \realnum^{\pcadim} \}$ using PCA. 

% in the $\frameth$-th frame of the $\videoth$-th video 

\section{Learning} 

\paragraph{Action Detection Forest.}
\label{sec/body/adflearn}
The action detection forest, $\forestd$, performs action categorisation and 3D pose clustering simultaneously. 
In each leaf node, the 3D pose vectors $\ppset$ are classified into the action class labels $\aset$ and cohering 3D pose vectors are grouped together.
For given data triplets $\fset$, $\ppset$ and $\aset$, we grow $\ntreed$ decision trees by recursively splitting the data into two child nodes, where candidate split functions are generated randomly, each compares a value in the feature vector $\feature$ with a threshold. The best split is chosen among the candidates by maximising a quality measure. The splitting process is performed until the new node reaches its maximum depth or minimum number of data points. Lastly, every leave node stores the votes that are required in Hough-voting for action detection during testing. 

%are grouped coherent to form a 3D pose cluster. 

%is essentially a Hough forest \cite{Gall2011}, where,

%use to classify parts and let parts vote for the center of atomatic actions in space and time. 
%hybrid of clssification and clustering forest learnt using

%for object detection  with classification capability. 
%A decision tree is grown by passing down a random subset of training data from the root node. 
%Candidate split functions are generated randomly, each compares a feature value with a threshold. 
%n a new node, the optimal split is chosen among the candidates by maximizing a quality measure. 
%Incoming training data are divided into two subsets, which are passed down to two new child nodes.  

%Both classification and detection are performed in a single decision tree. To this end,

We propose a new integrated quality measure $\qualityad(\cdot)$ at the node $\curnode$ as:
%is proposed in equation \ref{eq:newquality}:
%\begin{align}
%	\label{eq:newquality}
%	\qualityad(\ppset_{\curnode}) & = \alpha \infogain(\ppset_{\curnode}) + (1-\alpha)\qualityjr(\ppset_{\curnode}) \\ 
%	\qualityjr(\ppset_{\curnode}) & = \displaystyle\sum_{c = 1}^{\nclass} \Psi(\Sigma(\ppset_{\curnode c})) - \nodeweight_{L}\Psi(\Sigma(\ppset_{Lc})) - \nodeweight_{R}\Psi(\Sigma(\ppset_{Rc}))\\
%	\alpha & = \max_{c}(|\aset_{\curnode} = c| / |\aset-{\curnode}|)- \min_{c} (|\aset_\curnode = c| / | \aset_{\curnode}|)
%\end{align}
\begin{align}
	\label{eq:newquality}
	\qualityad(\ppset_{\curnode}) & = (1 - \omega)\infogain(\ppset_{\curnode}) + \omega\qualityjr(\ppset_{\curnode}). 
\end{align}
The first term, $\infogain(\cdot)$, is the information gain measure used in standard classification forest \cite{Breiman2001} (here for action classification); the second term measures the improvement in 3D pose coherence when a split is performed    
\begin{equation}
	\qualityjr(\ppset_{\curnode}) = \displaystyle\sum_{c = 1}^{\nclass} \Psi(\Sigma(\ppset_{\curnode c})) - \Psi(\Sigma(\ppset_{lc})) 
- \Psi(\Sigma(\ppset_{rc})),
\end{equation} 
where $\Psi(\cdot) = \log(\det(\cdot)) $ and $\Sigma(\ppset_{\curnode c})$ is the covariance matrix of the PCA-compressed 3D pose vectors  $\ppset_{\curnode c}$ of the n-th node ($l,r$ denote the left and right split respectively) and the $c$-th action class. These measures are weighted by $\omega$ that describes the class purity of a node as
\begin{equation}
	\omega = \max_{c}(|\aset_{\curnode c}| / |\aset_{\curnode}|)- \min_{c} (|\aset_{\curnode c}| / | \aset_{\curnode}|),
\end{equation} 
where $\aset_{\curnode}$ denotes the action labels of training data in node $\curnode$ and $\aset_{\curnode c}$ the action labels of node $\curnode$ and class $c$. The first quality term in (\ref{eq:newquality})  optimises action classification performance while the second term optimises pose clustering performance within a node. Initially, classification is preferred over clustering, $\infogain(\cdot)$ is dominant when class labels are evenly distributed. However, $\omega$ gradually increases as the tree grows, shifting the learning focus to clustering. 

%$\aset_{\curnode c} = \{\action_{\curnode} = c| \action_{\curnode} \in \aset_{\curnode}\}$. 

%Information gain $\infogain$ \cite{Breiman2001} is applied initially to optimize classification performance. When class entropy decreases in deeper nodes, e.g. more than $90\%$ of the training instances belong to the same label, the second quality measure $\qualityad$ is used instead to optimize for detection accuracy: 
%where $\ppset_{\curnode c}$ represents the training dataset in current node $\curnode$ with class label $c$, it is split into two subsets 
%$\ppset_{Lc}$ and $\ppset_{Rc}$ to the left and right child nodes. 
%The compactness of 3D poses in a node is measured by the log-determinant of covariance $\log|\Sigma(\cdot)|$. 
%The quality measure is the increase in node data compactness after the split, weighted by the node size ratio $\nodeweight_{L} = |\ppset^{\curnode}_{Lc}| / |\ppset_{\curnode c}|$ and $\nodeweight_{R} = 1 - \nodeweight_{L}$.

% a small subset of data remains in each leaf node.
Once the learning is completed, the class posterior of a leaf node $\termnode$ is obtained by:  
\begin{equation}
	\label{eq:dforest_treeposterior}
	P(\action = c| \termnode) = |\aset_{\termnode c}|/| \aset_{\termnode}| 
\end{equation}
The distribution of 3D poses, given a action class-label $c$, is modelled by a Gaussian 
$\normdist(\mu({\ppset}_{\termnode c}), \Sigma(\ppset_{\termnode c}))$.
A vote $\vote_{\termnode c}$ cast by the leaf node is a  pair  
$ \vote_{\termnode c}= ( \videoth_{\termnode c}, \frameth_{\termnode c}), c = 1,...,\nclass$ such that
\begin{equation}
	(\videoth_{\termnode c}, \frameth_{\termnode c}) =  
	\displaystyle\argmin_{(\videoth, \frameth)} || \_{\videoth\frameth\hypth} - \mu({\ppset}_{\termnode c}) ||_{2}
\end{equation}
$\videoth_{\termnode c}$ and $\frameth_{\termnode c}$ are the indices of the training sample that is the nearest neighbour of Gaussian mean $\mu(\ppset_{\termnode c})$. As a result, $\frameth$ is a temporal vote to the staring point of actions in sequence $\videoth$.

% = \sum_{i}^{|\ppset_{\termnode c}|} \ppset_{\termnode c i}/|\ppset_{\termnode c}|$.


%back-projected 3D poses of the above Gaussian means, such that:
%where the mean is approximated by its nearest neighbour in the training dataset.  


%\begin{equation}
%	\begin{array}{c}
%
%		\videoth_{c\termnode}, \frameth_{c\termnode}, \hypth_{c\termnode} =  
%		\displaystyle\argmin_{\videoth, \frameth, \hypth} || \_{\videoth\frameth\hypth} - \overline{\ppset}_{\termnode} ||_{2}  \\
%
%		P(\'| \termnode, \action_{\termnode} = c) = \normdist(\pbodypose',%\pbodypose_{\videoth_{c\termnode}\frameth_{c\termnode}\hypth_{c\termnode}},  \Sigma(\ppset_{c\termnode})) 
%
%	\end{array}
%\end{equation}


%\begin{figure}[ht]
%\begin{center}
%\vspace{5cm}
% %\includegraphics[width=0.8\linewidth]{egfigure.eps}
%\end{center}
%   \caption{Methods in details.}
%\label{fig/body/long}
%\end{figure*}

\paragraph{Cross-modality Regression Forest.}
\label{sec:jrflearn}
The cross-modality regression forests $\{\forestr^{(j)}|{j} = 1,\dots,\njoints\}$, inspired by \cite{Criminisi2011}, are learned to refine the 3D locations of joints. Each forest $\forestr^{(j)}$ contains $\ntreer$ trees that estimate the location of the $j$-th joint, trained independely from the dataset $\{ \pset^{(j)}, \fset, \aset\}$, where $\pset^{(j)}$ is the $j$-th joint's 3D coordinates in $\pset$. 
Split function candidates are generated in the same way as action detection using the feature vector $\feature$. 
%We assume that class esimates are given during testing by the detection forest, so . 
Although action class posteriors can be computed in the terminal nodes, it is not modelled in this method as the action detection forest $\forestd$ provides a better action recognition rate that helps the localisation accuracies of $\forestr^{(j)}$. Thus, $\qualityjr(\pset^{(j)}_{\curnode})$ is used instead to optimise the split functions for the localisation accuracy of the $j$-th joint. 
% ---- back up 
%\begin{equation}
	% \qualityjr(\pset^{(m)}_{\curnode c}) = \displaystyle\sum_{c = 1}^{\nclass} \left(  \log|\Sigma(\pset^{(m)}_{\curnode c})| - \nodeweight_{L}\log|\Sigma(\pset^{(y)}_{Lc})| - \nodeweight_{R}\log|\Sigma(\pset^{(m)}_{Rc})| \right)
%	\qualityjr(\pset^{(m)}_{\curnode}) = \displaystyle\sum_{c = 1}^{\nclass} \Psi(\Sigma(\pset_{\curnode c})) - %\nodeweight_{L}\Psi(\Sigma(\pset_{Lc})) - \nodeweight_{R}\Psi(\Sigma(\pset_{Rc}))
%\end{equation}
% ---- back up

% not neccessary
% where $\pset^{(m)}_{\curnode c}$, $\pset^{(m)}_{L c}$ and $\pset^{(m)}_{R c}$ are the training data in $\curnode$, its left child and right child respectively. 
% Since the action class labels are recognized in advanced via action detection, the quality function $\qualityjr$ aims to minimize the uncertainty in joint regression rather than classification error.
Tree growing is stopped when the current node is smaller than a certain size.
Upon completion of $\forestr^{(j)}$, the output of its leaf nodes $\termnode$ are described by the mean joint coordinates with respect to class label, given by $\mu(\pset^{(j)}_{\termnode c})$.  

%The mean $\overline{\pset^{(m)}_{\termnode c}}$ is not computed for class $c$ no data is present in $\termnode$.  


\section{Testing}

Video snippet, the basic unit required for action detection \cite{Schindler2008}, is a short sequence excerpted from the testing video, centered at time $\timenow$. A snippet $\snippetnow$ contains $\snippetlen$ frames such that $\snippetnow = \left\{ \sframe_{\timenow - \snippetlen/2}, \dots, \sframe_{\timenow + \snippetlen/2 - 1} \right\}$. The testing process starts with extracting features from $\snippetnow$, such that $\fset_{\snippetnow} = \{ \feature_{ij}\}$, where $i =\timenow-\snippetlen/2,...,\timenow+\snippetlen/2-1$ and $j=1,...,\nhyp$ with $\nhyp$ denotes the number of part configurations of the frame.
\paragraph{Action detection (classification).} 
\label{sec:adftest}
Action detection forest $\forestd$ performs action classification on $\snippetnow$. Let $\termnode_k[\feature_{ij}]$ be the leaf node reached by a feature $\feature_{ij}$ in the $k$-th tree of $\forestd$. The posterior of snippet action class at time $t$ is defined as 
\begin{equation}
	P(\action = c | \snippetnow, \forestd) = \sum_{k,i,j = 1}^{\ntreed, l, \nhyp}\frac {P(\action=c|\termnode_{k}[\feature_{ij}])} {\ntreed l \nhyp}. 
\end{equation} 
%Consequently, the pose found in $\sframe_{\timenow}$ can be estimated from $\snippetnow$ by action detection. 
\paragraph{Action detection (pose voting).}
A Hough-based voting scheme is designed for action detection. 
As mentioned in section \ref{sec/body/adflearn}, the vote $(p,q)$ stored in $\forestd$ are temporally associated with their corresponding action sequence and time frame in the training data set. Note each training frame is paired with a ground truth 3D pose. Hence, all frames $I$ can vote for a 3D pose at time $\timenow$, by applying temporal offsets  $\delta$ to the votes obtained from $\fset_{\snippetnow}$. We define a function $\Delta(\snippetnow,c)$ that returns a set of 3D pose estimates in $\pset$ for action label $c$ from Hough-voting during action detection: 
%The set of votes $\vset[\sframe_i] = \{ \videoth_{k c}[\sframe_{i}], \frameth_{k c}[\sframe_{i}]|k\!\in\!1,\dots,\nvset;c\!\in\!1,\dots,\nclass\}$ is obtained by passing down $\forestd$ the features extracted form a frame in snippet $\sframe_{i} \in \snippetnow$. 
\begin{equation}
	\Delta(\snippetnow, c) = \pset^{\Delta}_{\termnode_{k}[\feature_{(t+\delta)j}]c}
\end{equation}
where $ k = 1,...,\ntreed$, $\delta = -\snippetlen/2,...,\snippetlen/2-1$, $j = 1,...,\nhyp$. The set $\pset^{\Delta}_{\termnode_{k}[\feature_{(t+\delta)j}]c}$ denotes the $\delta$-voted (offset-ed) pose from the $\delta$-th frame in $\snippetnow$, \ie $(t\!+\!\delta)$-th frame in input video, by passing down the $k$-th tree in $\forestd$
%are the poses associated with the votes obtained from $\snippetnow$, such that
\begin{equation}
	\begin{split}
		& \pset^{\Delta}_{\termnode_{k}[\feature_{(t+\delta)j}]c} = \{\bodypose_{\videoth(\frameth-\delta)\hypth}\}\\ 
		\mbox{ s.t. } & (\videoth,\frameth) = \vote_{\termnode_{k}[\feature_{(t+\delta)j}]c} \mbox{ and } \action_{\videoth\frameth\hypth} = c 
\end{split} 
\end{equation} 

% ----------------- BACK UP 2
%\begin{equation}
%\pset^{\mathcal{D}}_{c} = \{ \bodypose^{\Delta}_{\termnode_{k}[\feature_{(t+\delta)j}]c}\}
%\end{equation}
%where $j = [1,\nhyp]$, $\delta = [-\snippetlen/2,\snippetlen/2]$,  $ k = [1,\ntreed]$. The set $\bodypose^{\Delta}_{\termnode_{k}[\feature_{(t+\delta)j}]c}$ denotes the $\delta$-voted (offseted) pose from the $\delta$-th frame in $\snippetnow$, \ie $(t\!+\!\delta)$-th frame in input video, by passing down the $k$-th tree in $\forestd$
%are the poses associated with the votes obtained from $\snippetnow$, such that
%\begin{equation}
%	\begin{split}
%		& \bodypose^{\Delta}_{\termnode_{k}[\feature_{(t+\delta)j}]c} = \bodypose_{\videoth(\frameth-\delta)\hypth}\\ 
%		\mbox{ s.t. } & \videoth,\frameth\in\vote_{\termnode_{k}[\feature_{(t+\delta)j}]c}, \aset_{\videoth} = c 
%\end{split} 
%\end{equation} 
%------------------ BACK UP 1
%\begin{equation}
%	\end{
%The set of all votes cast by $\snippet_\timenow$, in the 3D pose space is defined in equation \ref{eq:allDetectVotes}:
%\begin{equation}
%	\label{eq:allDetectVotes}
%	\begin{aligned}[lcl]
%		\vset_{\_{\timenow c}} & = & \{\vset^{-\delta}_{\termnode[\sframe_{t + \delta}]};\; \delta \in [-\timenow/2,\timenow/2 )\} \\
%		\vset^{-\delta}_{\termnode[\sframe_{t + \delta}]} & = & \{\bodypose_{\videoth_{c\termnode}\frameth_{c(\termnode-i)}\hypth_{c\termnode}}; \; c = i, \dots, \nclass \}
%	\end{aligned} 
%\end{equation}
Elements returned from $\Delta(\snippetnow, c)$ represent 3D pose estimations at time $\timenow$. A 3D pose $\bodyposead_{\timenow}$ is hence modelled by $\njoints$ independent Gaussians with respect to its joints. 
\begin{equation}
	\begin{split}
		& P( \bodyposead^{(j)}_{\timenow}| \snippetnow, \action = c,\forestd) \\ 
	 & = \normdist(\bodyposead^{(j)}_{\timenow}; \mu(\Delta^{(j)}(\snippetnow,c)), \Sigma(\Delta^{(j)}(\snippetnow,c))
\end{split} 
\end{equation}
where $\bodyposead_{\timenow}^{(j)}\in\realnum^{3}$ is the $j$-th joint in $\bodyposead_{\timenow}\in\realnum^{3\njoints}$.
% backup---------------------------
%\begin{equation}
%	\begin{split}
%		& P( \bodyposead^{(j)}_{\timenow}| \snippetnow, \action = c,\forestd) \\ 
%	 & = \normdist(\bodyposead^{(j)}_{\timenow}; \mu(\pset^{\mathcal{D}(j)}_{c}), \Sigma(\pset^{\mathcal{D}(j)}_{\timenow c}))
%\end{split} 
%\end{equation}
%where $\bodyposead_{\timenow}^{(j)}\in\realnum^{3}$ is the $j$-th joint in $\bodyposead_{\timenow}\in\realnum^{3\njoints}$.
% end of back up
\paragraph{Cross-modality Regression.}
\label{sec:jrftest}
Estimation of current 3D pose by the regression forest, $\bodyposepr_{\timenow}$, is performed on per-frame basis. 
Passing down features of current $t$th frame $\{\feature_{ti}|i = 1,...,\nhyp\}$, the set of pose estimates for class $c$ is returned by the function $\Phi(\snippetnow, c)$
\begin{equation}
	\label{eq:regressset}
	% back up! 
	%\pset^{\mathcal{R}(j)}_{c} = \{\mu({\pset^{(j)}_{\psi c}})|\psi \in \termnode[\feature \in \fset_{\snippetnow}]\}
	\Phi^{(j)}(\snippetnow, c)\!=\!\ \pset^{(j)}_{\termnode_{k}[\feature_{ti}]c},  i\!=\!1,...,\nhyp,k\!=\!1,...,\ntreer
\end{equation} 
The distribution of votes for the $j$-th joint is described by an Gaussian: 
\begin{equation}
	\begin{split}
		% & P( \bodyposepr_{\timenow}^{(j)}| \snippetnow, \action = c, \forestr)\\ 
		% & = \normdist(\bodyposepr_{\timenow}^{(j)};\mu(\pset^{\mathcal{R}(j)}_{c}),\Sigma(\pset^{\mathcal{R}(j)}_{c})
		& P( \bodyposepr_{\timenow}^{(j)}| \snippetnow, \action = c, \forestr)\\ 
		& = \normdist(\bodyposepr_{\timenow}^{(j)};\mu(\Phi^{(j)}(\snippetnow, c)),\Sigma(\Phi^{(j)}(\snippetnow,c))
\end{split}
\end{equation}




\paragraph{Combined Pose Estimation.}

Three-dimensional human poses are estimated globally via action detection, and locally by the joint regression forests. 
Assuming $\bodyposepr_{\timenow}$ and $\bodyposead_{\timenow}$ are independent, 
the probability of both observations coincide at $\bodyposez$ is 
\begin{equation}
	\label{eq:combination}
	\begin{aligned}[rl] 
		& P( \bodyposead_{\timenow}\!=\!\bodyposepr_{\timenow}\!=\!\bodyposez, \action\!=\!c| \snippetnow, \forestr, \forestd)\\ 
		= & P(\action\!=\!c | \snippetnow, \forestd) \prod_{j = 1}^{\njoints} P( \bodyposez^{(j)} | \snippetnow, \action\!=\!c, \forestr) \\
			 & \hspace{20mm} P( \bodyposez^{(j)}  | \snippetnow, \action\!=\!c, \forestd) \\
		% = &  P(\action = c | \_\timenow, \forestd) \prod_{j= 1} ^{\njoints}\normdist(\bodyposez_{\timenow}^{(j)};\:\overline{\rset^{(j)}_{\sframe_{\timenow}}},\Sigma(\rset^{(m)}_{\sframe_{\timenow}})) \normdist(\bodyposez^{(j)}_\timenow; \overline{\vset^{(m)}_{\snippet_\timenow}}, \Sigma(\vset^{(m)}_{\snippet_\timenow}))\\ 
		= &  P(\action\!=\!c | \snippetnow, \forestd) \prod_{j = 1}^{\njoints} \normdist(\bodyposez^{(j)}; \finalbodypose^{(j)}_{\timenow c}, \finalvariance^{(j)}_{\timenow c})
	\end{aligned}
\end{equation} 
% ---- back up --- 
%\begin{equation}
%	\label{eq:combination}
%	\begin{aligned}[rl] 
%		& P( \bodyposead_{\timenow}^{(m)} = \bodyposez^{(m)}_{\timenow}, \bodyposepr^{(m)}_{\timenow} = \bodyposez^{(m)}_{\timenow}, \action_{\timenow} = c| \_\timenow, \forestr, \forestd)\\ 
%		= & P( \bodyposead_{\timenow}^{(m)} | \bodyposepr^{(m)}_{\timenow}\_\timenow, \action_{\timenow} = c, \forestd, \forestr) P( \bodyposepr^{(m)}_\timenow | \snippet_\timenow, \action_{\timenow} = c, \forestd) P(\action_{\timenow} = c | \snippet_\timenow, \forestd) \\
%		= & P( \bodyposez_{\timenow}^{(m)} | \_\timenow, \action_{\timenow} = c, \forestr) P( \bodyposez^{(m)}_\timenow | \snippet_\timenow, \action_{\timenow} = c, \forestd) P(\action_{\timenow} = c | \snippet_\timenow, \forestd)\\ 
%		= & \normdist(\bodyposez_{\timenow}^{(m)};\:\overline{\rset^{(m)}_{\sframe_{\timenow}}},\Sigma(\rset^{(m)}_{\sframe_{\timenow}})) \normdist(\bodyposez^{(m)}_\timenow; \overline{\vset^{(m)}_{\_\timenow}}, \Sigma(\vset^{(m)}_{\snippet_\timenow})) P(\action_{\timenow} = c | \snippet_\timenow, \forestd)\\ 
%		= & \normdist(\bodyposez^{(m)}_{\timenow}; \mu^{(m)}_{\timenow}, \sigma^{m}_{\timenow}) P(\action_{\timenow} = c | \_\timenow, \forestd)
%	\end{aligned}
%\end{equation}
% --- back up --- 
Since the product of Gaussians is also a Gaussian, such that $\finalbodypose^{(j)}_{\timenow c}$ and $\finalvariance^{(j)}_{\timenow c}$ are 
\begin{equation}
	\label{eq:combination2}
	\begin{split}
		\finalbodypose^{(j)}_{\timenow c} = & \finalvariance^{(j)}_{\timenow c} 
		[
			\Sigma(\Phi^{(j)}(\snippetnow, c))^{-1} 
			\mu(\Delta^{(j)}(\snippetnow, c)) + \\ &  
			\Sigma(\Delta^{(j)}(\snippetnow, c))^{-1} 
			\mu(\Phi^{(j)}(\snippetnow, c))
			%\Sigma(\pset^{\mathcal{R}(j)}_{c})^{-1} 
			%\mu(\pset^{\mathcal{D}(j)}_{c}) + \\ &  
			%\Sigma(\pset^{\mathcal{D}(j)}_{c})^{-1} 
			%\mu(\pset^{\mathcal{R}(j)}_{c})
		] \\  
		\finalvariance^{(j)}_{\timenow c} = &  
		[
			%\Sigma(\pset^{\mathcal{R}(j)}_{c})^{-1} 
			%+ 
			%\Sigma(\pset^{\mathcal{D}(j)}_{c})^{-1} 
			\Sigma(\Phi^{(j)}(\snippetnow, c))^{-1}\!
			+\!
			\Sigma(\Delta^{(j)}(\snippetnow, c))^{-1} 
		]^{-1}\!
		\end{split}
	\end{equation}
	Consider the probability distribution in equation (\ref{eq:combination}), the final 3D pose estimation is described by the mean joint location $\finalbodypose^{(j)}_{t\hat{c}}$ of the most probable action category $\hat{c} = \argmax_c  P(\action = c | \snippetnow, \forestd)$, with the confidence region indicated by the covariance $\finalvariance^{(j)}_{t\hat{c}}$, where $ j=1,...,\njoints$. 

\label{sec:eval}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1.02\linewidth]{fig/body/errplot2d.pdf} 
 	\includegraphics[width=0.6\linewidth]{fig/body/plot_legend.pdf}
	\caption{2D joint localisation errors of our methods, \cite{Yang2011} and \cite{Eichner2012} their median smoothed versions.}
\label{fig/body/errorplot2d}
\end{figure} 

\section{Evaluation}
\label{sec/body/evaluation}

\subsection{The APE Evaluation Dataset}
Experiments were performed to investigate the feasibility of the proposed approach. 
Existing public 3D pose datasets are inadequate to justify the main objectives of the proposed approach. Whilst benchmarks such as \cite{Sigal2010, Yao2012} model joint angles rather than joint positions using sophisticated techniques, e.g. camera networks, from a static area, our framework focuses on flexible, multi-action 3D HPE from monocular videos without using background statistics.

To this end, we collected the \emph{action-pose-estimation (APE)} dataset for both quantitative and qualitative evaluations. The APE dataset contains $245$ sequences from $7$ subjects performing $7$ categories of actions. Videos of each subject were recorded in different environments, changing camera poses and moving background objects. The APE dataset will be made publicly available. 
%We refer the reader to \cite{APEwebsite} for details about the dataset.

The setting of APE dataset is considered challenging for traditional 3D HPE because: (1) no scene-dependent cues, \eg foreground segmentation, can be applied, (2) testing is done in unseen environments. 
Experiments are divided into two parts. In the first part, pose estimation accuracy was evaluated quantitatively with ground truth and current state-of-the-arts, in 3D and 2D respectively. In the second part, we demonstrated the knowledge transfer ability qualitatively, by testing with other videos and datasets.     


\subsection{Experimental Results}

\begin{figure}[ht]
\centering
	\includegraphics[width=0.8\linewidth]{fig/body/errplot3d.pdf} 
	\caption{3D joint localisation errors based on ground truth pose from Kinect sensor}
\label{fig/body/errorplot3d}
\end{figure}



\begin{figure}[ht]
	\centering
	\includegraphics[height=0.30\linewidth]{fig/body/confm_detection.pdf} \hspace{1cm} 
	\includegraphics[height=0.30\linewidth]{fig/body/confm_regression.pdf}
	\caption{Confusion matrices of action classification by (left) action detection forest, and (right) cross-modality regression forest} 
	\label{fig/body/confm}
\end{figure}

\paragraph{Quantitative Evaluation.}
\label{sec/body/quant}
The proposed approach was evaluated quantitatively using the leave-one-out cross-validation strategy. A subject was taken out in turn for testing, thus the model is trained with $210$ sequences and the remaining $35$ sequences are evaluated. 
Snippets are extracted densely from training and testing data, where $\snippetlen = 10$. 
Table \ref{tab/body/rf_train_params} lists the training parameters. 
%Parameters used in training are shown in table \ref{tab/body/rf_train_params}. 

\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline 
		Forest & \# tree & Max. depth & Min. node size \\ \hline 
		$\forestd$ & 10 & 16 & 15 \\ \hline 
		$\forestr$ & 10 & 14 & 20 \\ \hline 
	\end{tabular} 
	\caption{Parameters used in training $\forestd$ and $\forestr$.}
	\label{tab/body/rf_train_params}
\end{table}

\input{include/body_class3d}

Pose estimation accuracy was evaluated in both 2D and 3D. Accuracies of 3D joint coordinates were compared directly with ground truth 3D poses captured by the Kinect sensor. Accuracies in 2D were measured by back-projecting the poses to image coordinates.  
Besides the combined pose estimation $\finalbodypose$, we also evaluated each of the forests alone, and compared it with the latest 2D HPE algorithms \cite{Eichner2012} and \cite{Yang2011}. 
In order to cope with actions performed in different speeds, testing videos are preprocessed by normalising with respect to their action speeds estimated from the first $25$ frames of the videos.
In order to make a fair comparison, the joint coordinates from the frame-based algorithms \cite{Eichner2012} and \cite{Yang2011} are temporally smoothed by a $10$-frame median filter, as our approach estimates poses from multiple-frame snippets. 

Action classification rates of individual frames by both forests are presented in figure \ref{fig/body/confm}. 
The action detection forest achieves excellent accuracy, as it has been optimised for classification during learning, the video-based input, snippet, also provides temporal cues that improve classification. It complements the regression forests that focus on the localisation accuracy of joints.    
The average 3D joint localisation errors of the experiments were reported in figure \ref{fig/body/errorplot3d}. 
Sample results of the proposed method are also presented in figures \ref{fig/body/APE1}, \ref{fig/body/APE2} and \ref{fig/body/APE3}. 

The comparison our method and other 2D HPE algorithms is illustrated in figure \ref{fig/body/errorplot2d}.
The proposed framework showed promising results, by extending the flexibility of \cite{Yang2011}, the proposed method showed high robustness in 3D pose estimation and outperformed both state-of-the-arts in the 2D tests. The hand parts have the highest localisation errors because of their large movements and frequent occlusions, which are as well indicated by the big variance ellipsoids in figures \ref{fig/body/APE1}, \ref{fig/body/APE2}, \ref{fig/body/APE3}.

The per-class localisation errors are listed in table \ref{tab/body/errperclass3D} (3D) and \ref{tab/body/errperclass2D} (2D). While some classes reported significant improvements after combining the results of action detection and pose regression, \eg ``clap'' and ``wave 1'', the ``bend'' and ``box'' class reported the highest error rates.
The 2D part detections obtained from the ``box'' and ``bend'' classes are less accurate than those from other classes.  For the ``box'' action, self-occlusion happens frequently such that the part detector is confused about the left and right hand positions, making the hand position distributions spread around the torso as in figure \ref{fig/body/APEerr2} and \ref{fig/body/APEerr3}. 
Similarly, when the arms are stretched overhead and occluded, the 2D DPM model used in the experiments gives incorrect results.  

\input{include/body_ape.tex}

\paragraph{Qualitative Evaluation.}
Knowledge transfer was evaluated by reusing the models trained in section \ref{sec/body/quant} to other datasets without retraining.
The KTH \cite{Schuldt2004} and Weizmann \cite{Gorelick2007} dataset were used in the experiments as they shared action categories with the APE dataset. 

The experimental results are reported qualitatively in figure \ref{fig/body/otherresults}. Even though the input videos are of extremely low resolutions, rendering them inapplicable to typical 3D HPE methods, our framework is still able to estimate their actions and poses simultaneously with encouraging accuracy. 
Incorrect poses were estimated when too many false positive parts are detected from the low resolution images, \eg figure \ref{fig/body/otherresults}(g--h). 

\paragraph{Discussion.}
The experimental results have demonstrated high feasibility in the idea of using action detection to estimate 3D poses under challenging conditions.  
Coupling the outputs from the random forests, the 3D pose estimation accuracy is further enhanced. Action detection gives a global pose estimation and the corresponding class label. Meanwhile, errors in the initial global estimation, due to the differences among individual action patterns, are corrected locally by regression forests, which improves the accuracy of the final pose estimation as shown in figure \ref{fig/body/errorplot3d}. 

On the other side, there is still room for improvement in the proposed approach. The proposed method relies on DPM as the only source of input. Albeit great flexibility, \cf \cite{Yang2011}, the performance of a DPM depends on its training data. Our method handles minor errors gracefully by allowing multiple hypotheses and snippet-based input, but large errors cannot be recovered completely, \eg figure \ref{fig/body/APEerr1} and \ref{fig/body/APEerr2} for APE dataset, and \ref{fig/body/others/g} and \ref{fig/body/others/h} for KTH and Weizmann dataset respectively. 
Besides, the proposed method runs at $0.31$fps. Feature extraction from DPM is the run-time bottleneck. The pose estimatior alone runs at about $5$fps by precomputing the DPM features. 

\input{include/body_others.tex}

\section{Discussion}
\label{sec/body/conclusions}

The challenging problem of 3D human pose estimation is discussed in this chapter. 
While traditional methods for 3D human pose estimation emphasise accuracy over their compatibility with realistic applications, we present a novel practical approach without using any scene-dependent constraints.
We investigate the new area of using action for pose estimation. The proposed method combines human action detection and deformable part model-based 2D human pose estimation to estimate 3D poses from unconstrained, monocular videos.  
The new APE dataset is introduced to evaluate the feasibility of our approach.  
Experimental results have shown promising results and also high flexibility by transferring the knowledge obtained from training data to other unseen datasets.
In the future we plan to apply kinematic constraints in our system for pose refinement. 
We suggest that the collaboration between the techniques in human action and pose analysis will be beneficial to both areas of computer vision research in the coming future.  

% Add limitations of the current approach 
